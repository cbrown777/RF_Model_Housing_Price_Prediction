---
title: "Lab 7"
author: "Connor Brown"
output: pdf_document
date: "11:59PM March 31, 2019"
---

Generate $\mathbb{D}$ with $n = 100$ and $p = 1$ where $x$ is created from iid realizations from a standard uniform, $y$ comes from $f(x) = 3 - 4x$ and $\epsilon$ are iid realizations from a T distribution with 10 degrees of freedom.

```{r}
set.seed(1997)
n = 100
p = 1
X = matrix(runif(n) , ncol = 1)
f_x = 3 - 4 * X
y = f_x + rt(n, df = 10)
```

Run the linear model using `lm` and repeat RMSE and $R^2$.

```{r}

linear_mod = lm(y ~ X)
coef(linear_mod)
summary(linear_mod)$sigma
summary(linear_mod)$r.squared

```

Progressively add columns of X (as draws from a standard uniform), run the linear model, and show $R^2$ goes to 1 and $s_e$ goes to zero. Save the $s_e$ in a vector called `in_sample_s_e`.

```{r}
in_sample_s_e = array(NA, n - 2)
linear_mods = list()
for (j in 1 : (n - 2)){
  X = cbind(X, runif(n))
  linear_mods[[j]] = lm(y ~ ., data.frame(X))
  in_sample_s_e[j] = sd(linear_mods[[j]]$residuals)
}
dim(X)
summary(linear_mods[[j]])$r.squared
in_sample_s_e
d = diff(in_sample_s_e)
all(d < 0)
```

Compute a corresponding vector `oos_s_e` and show that it is increasing (for the most part) in degrees of freedom.

```{r}
n_star = 1e5
p = 1
X_star = matrix(runif(n_star) , ncol = 1)
f_x_star = 3 - 4 * X_star
y_star = f_x_star + rt(n_star, df = 10)
oos_s_e = array(NA, n - 2)
for (j in 1 : (n - 2)){
  X_star = cbind(X_star, runif(n_star))
  y_hat_star = predict(linear_mods[[j]], data.frame(X_star)) 
  oos_s_e[j] = sd(y_star - y_hat_star)
}
oos_s_e
d = diff(oos_s_e)
sum(d > 0)
```

Validate the linear model for the Boston housing data.

```{r}

Xy = MASS::Boston
K = 10
test_indices = sample(1 : nrow(Xy), 1 / K * nrow(Xy))
train_indices = setdiff(1 : nrow(Xy), test_indices)

Xy_train = Xy[train_indices, ]
Xy_test = Xy[test_indices, ]

lin_mod = lm(medv ~ ., Xy_train)
summary(lin_mod)$r.squared #In-Sample R^2
sd(lin_mod$residuals)     #In-sample SE

y_hat_test = predict(lin_mod, Xy_test)
oos_residuals = Xy_test$medv - y_hat_test
1 - sum(oos_residuals^2) / sum((Xy_test$medv - mean(Xy_test$medv))^2) #OOS R^2
sd(oos_residuals) #OOS SE

```


Let $x$ be iid realizations from a $U(0, 5)$, $y$ comes from $f(x) = 3 - 4x + 2x^2$ and $\epsilon$ are iid realizations from a standard normal distribution. With no limit on the number of samples you cant take, use regular OLS _without a quadratic term_, find the true $h^*(x)$ (there will be no sampling variability at $n \rightarrow \infty$ and find the oos variance of the residuals.
```{r}
n = 10000
x = runif(n, 0, 5)
f_x = 3 - 4*x + 2*(x^2)
epsilons = rnorm(n, 0 , 1)
y = f_x + epsilons
X = (cbind(1, x))
plot(x, y)


K = 5 #i.e. the test set is 1/5th of the entire historical dataset
test_indices = sample(1 : n, (1 / K) * n) #sample random indices for your Dtest
train_indices = setdiff(1 : n, test_indices)#Gets the compliment indices for Dtrain

#Pull out the matrices and vectors based on the indices
X_train = X[train_indices, ]
y_train = y[train_indices]
X_test = X[test_indices, ]
y_test = y[test_indices]

#Now let's fit the model $g$ to the training data and compute in-sample error metrics:
mod = lm(y_train ~ ., data.frame(X_train)) #Creates model g, using Xtrain
coef(mod)
summary(mod)$r.squared #In-Sample R^2
sd(mod$residuals)     #In-sample SE

#Now let's see how we do on the test data. We compute $R^2$ and $s_e$ out of sample:
y_hat_oos = predict(mod, data.frame(X_test)) #Predicts y_hats using g with Xtest
oos_residuals = y_test - y_hat_oos           #y_tests - y_hats
1 - sum(oos_residuals^2) / sum((y_test - mean(y_test))^2) #OOS R^2
sd(oos_residuals) #OOS SE


h_star_x = predict(mod, data.frame(X))
```
Was there any overfitting in the previous exercise?

No there wasn't, p is too small to overfit. 


Find the error due to misspecification and due to ignorance expressed as variance of components of the residuals.
```{r}

misspec_error = f_x - h_star_x
var(misspec_error)

```
At $n = 100$, find the error due to estimation, due to misspecification and due to ignorance expressed as variance of components of the residuals.
```{r}
n = 100
x = runif(n, 0, 5)
f_x = 3 - 4*x + 2*(x^2)
epsilons = rnorm(n, 0 , 1)
y = f_x + epsilons
X = (cbind(1, x))
plot(x, y)

K = 5 #i.e. the test set is 1/5th of the entire historical dataset
test_indices = sample(1 : n, (1 / K) * n) #sample random indices for your Dtest
train_indices = setdiff(1 : n, test_indices)#Gets the compliment indices for Dtrain

#Pull out the matrices and vectors based on the indices
X_train = X[train_indices, ]
y_train = y[train_indices]
X_test = X[test_indices, ]
y_test = y[test_indices]

#Now let's fit the model $g$ to the training data and compute in-sample error metrics:
mod = lm(y_train ~ ., data.frame(X_train)) #Creates model g, using Xtrain

#Now let's see how we do on the test data. We compute $R^2$ and $s_e$ out of sample:
y_hat_oos = predict(mod, data.frame(X_test)) #Predicts y_hats using g with Xtest

h_star_x2 = predict(mod, data.frame(X))


est_error = h_star_x - h_star_x2
var(est_error)

misspec_error = f_x - h_star_x
var(misspec_error)
```
Do the variances add up to the total variance of the residual?
```{r}
#Yes, they do 
```
Validate the linear model for the Boston housing data where each feature is also modeled with a squared feature.
```{r}
rm(list = ls())
Boston = MASS::Boston
y = Boston[,14]
Boston = Boston[ , seq(1,13)]
n = nrow(Boston)

B_sqrd = Boston^2
Boston = cbind(Boston, B_sqrd)

K = 5 #i.e. the test set is 1/5th of the entire historical dataset
test_indices = sample(1 : n, (1 / K) * n) #sample random indices for your Dtest
train_indices = setdiff(1 : n, test_indices)#Gets the compliment indices for Dtrain

#Pull out the matrices and vectors based on the indices
X_train = Boston[train_indices, ]
y_train = y[train_indices]
X_test = Boston[test_indices, ]
y_test = y[test_indices]

#Now let's fit the model $g$ to the training data and compute in-sample error metrics:
mod = lm(y_train ~ ., data.frame(X_train)) #Creates model g, using Xtrain
coef(mod)
summary(mod)$r.squared #In-Sample R^2
sd(mod$residuals)     #In-sample SE

#Now let's see how we do on the test data. We compute $R^2$ and $s_e$ out of sample:
y_hat_oos = predict(mod, data.frame(X_test)) #Predicts y_hats using g with Xtest
oos_residuals = y_test - y_hat_oos           #y_tests - y_hats
1 - sum(oos_residuals^2) / sum((y_test - mean(y_test))^2) #OOS R^2
sd(oos_residuals) #OOS SE

```
Validate the linear model for the Boston housing data where each feature is also modeled with a squared feature and a cubed feature.
```{r}
rm(list = ls())
Boston = MASS::Boston
y = Boston[, 14]
Boston = Boston[ , seq(1,13)]
n = nrow(Boston)

B_sqrd = Boston^2
B_cbd = Boston^3
Boston = cbind(Boston, B_sqrd, B_cbd)

K = 5 #i.e. the test set is 1/5th of the entire historical dataset
test_indices = sample(1 : n, (1 / K) * n) #sample random indices for your Dtest
train_indices = setdiff(1 : n, test_indices)#Gets the compliment indices for Dtrain

#Pull out the matrices and vectors based on the indices
X_train = Boston[train_indices, ]
y_train = y[train_indices]
X_test = Boston[test_indices, ]
y_test = y[test_indices]

#Now let's fit the model $g$ to the training data and compute in-sample error metrics:
mod = lm(y_train ~ ., data.frame(X_train)) #Creates model g, using Xtrain
coef(mod)
summary(mod)$r.squared #In-Sample R^2
sd(mod$residuals)     #In-sample SE

#Now let's see how we do on the test data. We compute $R^2$ and $s_e$ out of sample:
y_hat_oos = predict(mod, data.frame(X_test)) #Predicts y_hats using g with Xtest
oos_residuals = y_test - y_hat_oos           #y_tests - y_hats
1 - sum(oos_residuals^2) / sum((y_test - mean(y_test))^2) #OOS R^2
sd(oos_residuals) #OOS SE

```
Validate the linear model for the Boston housing data where each feature is also modeled with a squared feature and a cubed feature and a log(x + 1) feature and an exponential feature. 
```{r}
rm(list = ls())
Boston = MASS::Boston
y = Boston[, 14]
Boston = Boston[ , seq(1,13)]
n = nrow(Boston)

B_sqrd = Boston^2
B_cbd = Boston^3
B_logd = log(Boston) + 1
B_exp = exp(Boston)
Boston = cbind(Boston, B_sqrd, B_cbd, B_logd, B_exp)

K = 5 #i.e. the test set is 1/5th of the entire historical dataset
test_indices = sample(1 : n, (1 / K) * n) #sample random indices for your Dtest
train_indices = setdiff(1 : n, test_indices)#Gets the compliment indices for Dtrain

#Pull out the matrices and vectors based on the indices
X_train = Boston[train_indices, ]
y_train = y[train_indices]
X_test = Boston[test_indices, ]
y_test = y[test_indices]

#Now let's fit the model $g$ to the training data and compute in-sample error metrics:
mod = lm(y_train ~ ., data.frame(X_train)) #Creates model g, using Xtrain
coef(mod)
summary(mod)$r.squared #In-Sample R^2
sd(mod$residuals)     #In-sample SE

#Now let's see how we do on the test data. We compute $R^2$ and $s_e$ out of sample:
y_hat_oos = predict(mod, data.frame(X_test)) #Predicts y_hats using g with Xtest
oos_residuals = y_test - y_hat_oos           #y_tests - y_hats
1 - sum(oos_residuals^2) / sum((y_test - mean(y_test))^2) #OOS R^2
sd(oos_residuals) #OOS SE

```
Why do we need to log $x + 1$? Why not use log(x)?
log(0) is undefined 


